{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate porter\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction import text\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "seed = 1024\n",
    "np.random.seed(seed)\n",
    "path = ''\n",
    "\n",
    "train = pd.read_csv(path+\"train.csv\")\n",
    "test = pd.read_csv(path+\"test.csv\")\n",
    "\n",
    "def stem_str(x,stemmer=SnowballStemmer('english')):\n",
    "    x = text.re.sub(\"[^a-zA-Z0-9]\",\" \", x)\n",
    "    x = (\" \").join([stemmer.stem(z) for z in x.split(\" \")])\n",
    "    x = \" \".join(x.split())\n",
    "    return x\n",
    "\n",
    "porter = PorterStemmer()\n",
    "snowball = SnowballStemmer('english')\n",
    "\n",
    "\n",
    "print('Generate porter')\n",
    "train['question1_porter'] = train['question1'].astype(str).apply(lambda x:stem_str(x.lower(),porter))\n",
    "test['question1_porter'] = test['question1'].astype(str).apply(lambda x:stem_str(x.lower(),porter))\n",
    "\n",
    "train['question2_porter'] = train['question2'].astype(str).apply(lambda x:stem_str(x.lower(),porter))\n",
    "test['question2_porter'] = test['question2'].astype(str).apply(lambda x:stem_str(x.lower(),porter))\n",
    "\n",
    "train.to_csv(path+'train_porter.csv')\n",
    "test.to_csv(path+'test_porter.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate intersection\n",
      "Generate porter intersection\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder,LabelEncoder,StandardScaler\n",
    "from sklearn.decomposition import TruncatedSVD,PCA\n",
    "from sklearn.metrics.pairwise import cosine_similarity, pairwise_distances\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "seed = 1024\n",
    "np.random.seed(seed)\n",
    "path = \"\"\n",
    "\n",
    "train = pd.read_csv(path+\"train_porter.csv\")\n",
    "test = pd.read_csv(path+\"test_porter.csv\")\n",
    "test['is_duplicated']=[-1]*test.shape[0]\n",
    "\n",
    "len_train = train.shape[0]\n",
    "\n",
    "data_all = pd.concat([train,test])\n",
    "\n",
    "\n",
    "def calc_set_intersection(text_a, text_b):\n",
    "    a = set(text_a.split())\n",
    "    b = set(text_b.split())\n",
    "    return len(a.intersection(b)) *1.0 / len(a)\n",
    "\n",
    "print('Generate intersection')\n",
    "train_interaction = train.astype(str).apply(lambda x:calc_set_intersection(x['question1'],x['question2']),axis=1)\n",
    "test_interaction = test.astype(str).apply(lambda x:calc_set_intersection(x['question1'],x['question2']),axis=1)\n",
    "pd.to_pickle(train_interaction,path+\"train_interaction.pkl\")\n",
    "pd.to_pickle(test_interaction,path+\"test_interaction.pkl\")\n",
    "\n",
    "print('Generate porter intersection')\n",
    "train_porter_interaction = train.astype(str).apply(lambda x:calc_set_intersection(x['question1_porter'],x['question2_porter']),axis=1)\n",
    "test_porter_interaction = test.astype(str).apply(lambda x:calc_set_intersection(x['question1_porter'],x['question2_porter']),axis=1)\n",
    "\n",
    "pd.to_pickle(train_porter_interaction,path+\"train_porter_interaction.pkl\")\n",
    "pd.to_pickle(test_porter_interaction,path+\"test_porter_interaction.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate tfidf\n",
      "Generate porter tfidf\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "seed = 1024\n",
    "np.random.seed(seed)\n",
    "path = \"/input/\"\n",
    "\n",
    "ft = ['question1','question2','question1_porter','question2_porter']\n",
    "train = pd.read_csv(path+\"train_porter.csv\")[ft]\n",
    "test = pd.read_csv(path+\"test_porter.csv\")[ft]\n",
    "# test['is_duplicated']=[-1]*test.shape[0]\n",
    "\n",
    "len_train = train.shape[0]\n",
    "\n",
    "data_all = pd.concat([train,test])\n",
    "#print (data_all)\n",
    "\n",
    "max_features = None\n",
    "ngram_range = (1,2)\n",
    "min_df = 3\n",
    "print('Generate tfidf')\n",
    "feats= ['question1','question2']\n",
    "vect_orig = TfidfVectorizer(max_features=max_features,ngram_range=ngram_range, min_df=min_df)\n",
    "\n",
    "corpus = []\n",
    "for f in feats:\n",
    "    data_all[f] = data_all[f].astype(str)\n",
    "    corpus+=data_all[f].values.tolist()\n",
    "\n",
    "vect_orig.fit(\n",
    "    corpus\n",
    "    )\n",
    "\n",
    "for f in feats:\n",
    "    tfidfs = vect_orig.transform(data_all[f].values.tolist())\n",
    "    train_tfidf = tfidfs[:train.shape[0]]\n",
    "    test_tfidf = tfidfs[train.shape[0]:]\n",
    "    pd.to_pickle(train_tfidf,'train_%s_tfidf.pkl'%f)\n",
    "    pd.to_pickle(test_tfidf,'test_%s_tfidf.pkl'%f)\n",
    "\n",
    "\n",
    "print('Generate porter tfidf')\n",
    "feats= ['question1_porter','question2_porter']\n",
    "vect_orig = TfidfVectorizer(max_features=max_features,ngram_range=ngram_range, min_df=min_df)\n",
    "\n",
    "corpus = []\n",
    "for f in feats:\n",
    "    data_all[f] = data_all[f].astype(str)\n",
    "    corpus+=data_all[f].values.tolist()\n",
    "\n",
    "vect_orig.fit(\n",
    "    corpus\n",
    "    )\n",
    "\n",
    "for f in feats:\n",
    "    tfidfs = vect_orig.transform(data_all[f].values.tolist())\n",
    "    train_tfidf = tfidfs[:train.shape[0]]\n",
    "    test_tfidf = tfidfs[train.shape[0]:]\n",
    "    pd.to_pickle(train_tfidf,'train_%s_tfidf.pkl'%f)\n",
    "    pd.to_pickle(test_tfidf,'test_%s_tfidf.pkl'%f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Downloading nltk-3.2.2.tar.gz (1.2MB)\n",
      "\u001b[K    100% |████████████████████████████████| 1.2MB 877kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.5/site-packages (from nltk)\n",
      "Building wheels for collected packages: nltk\n",
      "  Running setup.py bdist_wheel for nltk ... \u001b[?25l-\b \b\\\b \b|\b \bdone\n",
      "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/42/b5/27/718985cd9719e8a44a405d264d98214c7a607fb65f3a006f28\n",
      "Successfully built nltk\n",
      "Installing collected packages: nltk\n",
      "Successfully installed nltk-3.2.2\n",
      "Requirement already satisfied: distance in /usr/local/lib/python3.5/site-packages\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install nltk\n",
    "!pip install distance\n",
    "import nltk\n",
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate len\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.preprocessing import OneHotEncoder,LabelEncoder,StandardScaler\n",
    "from sklearn.decomposition import TruncatedSVD,PCA\n",
    "from sklearn.metrics.pairwise import cosine_similarity, pairwise_distances\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import distance\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "seed = 1024\n",
    "np.random.seed(seed)\n",
    "path = '/input/'\n",
    "\n",
    "train = pd.read_csv(path+\"train_porter.csv\").astype(str)\n",
    "test = pd.read_csv(path+\"test_porter.csv\").astype(str)\n",
    "\n",
    "\n",
    "def str_abs_diff_len(str1, str2):\n",
    "    return abs(len(str1)-len(str2))\n",
    "\n",
    "def str_len(str1):\n",
    "    return len(str(str1))\n",
    "\n",
    "def char_len(str1):\n",
    "    str1_list = set(str(str1).replace(' ',''))\n",
    "    return len(str1_list)\n",
    "\n",
    "def word_len(str1):\n",
    "    str1_list = str1.split(' ')\n",
    "    return len(str1_list)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "def word_match_share(row):\n",
    "    q1words = {}\n",
    "    q2words = {}\n",
    "    for word in str(row['question1']).lower().split():\n",
    "        if word not in stop_words:\n",
    "            q1words[word] = 1\n",
    "    for word in str(row['question2']).lower().split():\n",
    "        if word not in stop_words:\n",
    "            q2words[word] = 1\n",
    "    if len(q1words) == 0 or len(q2words) == 0:\n",
    "        # The computer-generated chaff includes a few questions that are nothing but stopwords\n",
    "        return 0\n",
    "    shared_words_in_q1 = [w for w in q1words.keys() if w in q2words]\n",
    "    shared_words_in_q2 = [w for w in q2words.keys() if w in q1words]\n",
    "    R = (len(shared_words_in_q1) + len(shared_words_in_q2))*1.0/(len(q1words) + len(q2words))\n",
    "    return R\n",
    "\n",
    "print('Generate len')\n",
    "feats = []\n",
    "\n",
    "train['abs_diff_len'] = train.apply(lambda x:str_abs_diff_len(x['question1'],x['question2']),axis=1)\n",
    "test['abs_diff_len']= test.apply(lambda x:str_abs_diff_len(x['question1'],x['question2']),axis=1)\n",
    "feats.append('abs_diff_len')\n",
    "\n",
    "train['R']=train.apply(word_match_share, axis=1, raw=True)\n",
    "test['R']=test.apply(word_match_share, axis=1, raw=True)\n",
    "feats.append('R')\n",
    "\n",
    "train['common_words'] = train.apply(lambda x: len(set(str(x['question1']).lower().split()).intersection(set(str(x['question2']).lower().split()))), axis=1)\n",
    "test['common_words'] = test.apply(lambda x: len(set(str(x['question1']).lower().split()).intersection(set(str(x['question2']).lower().split()))), axis=1)\n",
    "feats.append('common_words')\n",
    "\n",
    "for c in ['question1','question2']:\n",
    "    train['%s_char_len'%c] = train[c].apply(lambda x:char_len(x))\n",
    "    test['%s_char_len'%c] = test[c].apply(lambda x:char_len(x))\n",
    "    feats.append('%s_char_len'%c)\n",
    "\n",
    "    train['%s_str_len'%c] = train[c].apply(lambda x:str_len(x))\n",
    "    test['%s_str_len'%c] = test[c].apply(lambda x:str_len(x))\n",
    "    feats.append('%s_str_len'%c)\n",
    "    \n",
    "    train['%s_word_len'%c] = train[c].apply(lambda x:word_len(x))\n",
    "    test['%s_word_len'%c] = test[c].apply(lambda x:word_len(x))\n",
    "    feats.append('%s_word_len'%c)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "pd.to_pickle(train[feats].values,\"train_len.pkl\")\n",
    "pd.to_pickle(test[feats].values,\"test_len.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate jaccard\n",
      "Generate porter jaccard\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder,LabelEncoder,StandardScaler\n",
    "from sklearn.decomposition import TruncatedSVD,PCA\n",
    "from sklearn.metrics.pairwise import cosine_similarity, pairwise_distances\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import distance\n",
    "seed = 1024\n",
    "np.random.seed(seed)\n",
    "path = \"/input/\"\n",
    "\n",
    "train = pd.read_csv(path+\"train_porter.csv\")\n",
    "test = pd.read_csv(path+\"test_porter.csv\")\n",
    "test['is_duplicated']=[-1]*test.shape[0]\n",
    "\n",
    "len_train = train.shape[0]\n",
    "\n",
    "data_all = pd.concat([train,test])\n",
    "\n",
    "def str_jaccard(str1, str2):\n",
    "\n",
    "\n",
    "    str1_list = str1.split(\" \")\n",
    "    str2_list = str2.split(\" \")\n",
    "    res = distance.jaccard(str1_list, str2_list)\n",
    "    return res\n",
    "\n",
    "# shortest alignment\n",
    "def str_levenshtein_1(str1, str2):\n",
    "\n",
    "\n",
    "    str1_list = str1.split(' ')\n",
    "    str2_list = str2.split(' ')\n",
    "    res = distance.nlevenshtein(str1, str2,method=1)\n",
    "    return res\n",
    "\n",
    "# longest alignment\n",
    "def str_levenshtein_2(str1, str2):\n",
    "\n",
    "    str1_list = str1.split(' ')\n",
    "    str2_list = str2.split(' ')\n",
    "    res = distance.nlevenshtein(str1, str2,method=2)\n",
    "    return res\n",
    "\n",
    "def str_sorensen(str1, str2):\n",
    "\n",
    "    str1_list = str1.split(' ')\n",
    "    str2_list = str2.split(' ')\n",
    "    res = distance.sorensen(str1_list, str2_list)\n",
    "    return res\n",
    "\n",
    "print('Generate jaccard')\n",
    "train_jaccard = train.astype(str).apply(lambda x:str_jaccard(x['question1'],x['question2']),axis=1)\n",
    "test_jaccard = test.astype(str).apply(lambda x:str_jaccard(x['question1'],x['question2']),axis=1)\n",
    "pd.to_pickle(train_jaccard,\"train_jaccard.pkl\")\n",
    "pd.to_pickle(test_jaccard,\"test_jaccard.pkl\")\n",
    "\n",
    "print('Generate porter jaccard')\n",
    "train_porter_jaccard = train.astype(str).apply(lambda x:str_jaccard(x['question1_porter'],x['question2_porter']),axis=1)\n",
    "test_porter_jaccard = test.astype(str).apply(lambda x:str_jaccard(x['question1_porter'],x['question2_porter']),axis=1)\n",
    "\n",
    "pd.to_pickle(train_porter_jaccard,\"train_porter_jaccard.pkl\")\n",
    "pd.to_pickle(test_porter_jaccard,\"test_porter_jaccard.pkl\")\n",
    "\n",
    "\n",
    "# print('Generate levenshtein_1')\n",
    "# train_levenshtein_1 = train.astype(str).apply(lambda x:str_levenshtein_1(x['question1'],x['question2']),axis=1)\n",
    "# test_levenshtein_1 = test.astype(str).apply(lambda x:str_levenshtein_1(x['question1'],x['question2']),axis=1)\n",
    "# pd.to_pickle(train_levenshtein_1,path+\"train_levenshtein_1.pkl\")\n",
    "# pd.to_pickle(test_levenshtein_1,path+\"test_levenshtein_1.pkl\")\n",
    "\n",
    "# print('Generate porter levenshtein_1')\n",
    "# train_porter_levenshtein_1 = train.astype(str).apply(lambda x:str_levenshtein_1(x['question1_porter'],x['question2_porter']),axis=1)\n",
    "# test_porter_levenshtein_1 = test.astype(str).apply(lambda x:str_levenshtein_1(x['question1_porter'],x['question2_porter']),axis=1)\n",
    "\n",
    "# pd.to_pickle(train_porter_levenshtein_1,path+\"train_porter_levenshtein_1.pkl\")\n",
    "# pd.to_pickle(test_porter_levenshtein_1,path+\"test_porter_levenshtein_1.pkl\")\n",
    "\n",
    "\n",
    "# print('Generate levenshtein_2')\n",
    "# train_levenshtein_2 = train.astype(str).apply(lambda x:str_levenshtein_2(x['question1'],x['question2']),axis=1)\n",
    "# test_levenshtein_2 = test.astype(str).apply(lambda x:str_levenshtein_2(x['question1'],x['question2']),axis=1)\n",
    "# pd.to_pickle(train_levenshtein_2,path+\"train_levenshtein_2.pkl\")\n",
    "# pd.to_pickle(test_levenshtein_2,path+\"test_levenshtein_2.pkl\")\n",
    "\n",
    "# print('Generate porter levenshtein_2')\n",
    "# train_porter_levenshtein_2 = train.astype(str).apply(lambda x:str_levenshtein_2(x['question1_porter'],x['question2_porter']),axis=1)\n",
    "# test_porter_levenshtein_2 = test.astype(str).apply(lambda x:str_levenshtein_2(x['question1_porter'],x['question2_porter']),axis=1)\n",
    "\n",
    "# pd.to_pickle(train_porter_levenshtein_2,path+\"train_porter_levenshtein_2.pkl\")\n",
    "# pd.to_pickle(test_porter_levenshtein_2,path+\"test_porter_levenshtein_2.pkl\")\n",
    "\n",
    "\n",
    "# print('Generate sorensen')\n",
    "# train_sorensen = train.astype(str).apply(lambda x:str_sorensen(x['question1'],x['question2']),axis=1)\n",
    "# test_sorensen = test.astype(str).apply(lambda x:str_sorensen(x['question1'],x['question2']),axis=1)\n",
    "# pd.to_pickle(train_sorensen,path+\"train_sorensen.pkl\")\n",
    "# pd.to_pickle(test_sorensen,path+\"test_sorensen.pkl\")\n",
    "\n",
    "# print('Generate porter sorensen')\n",
    "# train_porter_sorensen = train.astype(str).apply(lambda x:str_sorensen(x['question1_porter'],x['question2_porter']),axis=1)\n",
    "# test_porter_sorensen = test.astype(str).apply(lambda x:str_sorensen(x['question1_porter'],x['question2_porter']),axis=1)\n",
    "\n",
    "# pd.to_pickle(train_porter_sorensen,path+\"train_porter_sorensen.pkl\")\n",
    "# pd.to_pickle(test_porter_sorensen,path+\"test_porter_sorensen.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/site-packages/ipykernel/__main__.py:29: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n",
      "/usr/local/lib/python3.5/site-packages/ipykernel/__main__.py:30: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n",
      "/usr/local/lib/python3.5/site-packages/ipykernel/__main__.py:32: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n",
      "/usr/local/lib/python3.5/site-packages/ipykernel/__main__.py:33: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n",
      "/usr/local/lib/python3.5/site-packages/ipykernel/__main__.py:36: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n",
      "/usr/local/lib/python3.5/site-packages/ipykernel/__main__.py:37: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n",
      "/usr/local/lib/python3.5/site-packages/ipykernel/__main__.py:39: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n",
      "/usr/local/lib/python3.5/site-packages/ipykernel/__main__.py:40: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(404290, 3073539)\n",
      "(2345796, 3073539)\n",
      "0.191269277687\n",
      "0.191144081052\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import sparse as ssp\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.datasets import dump_svmlight_file,load_svmlight_file\n",
    "from sklearn.utils import resample,shuffle\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "seed=1024\n",
    "np.random.seed(seed)\n",
    "path = \"/input/\"\n",
    "train = pd.read_csv(path+\"train_porter.csv\")\n",
    "\n",
    "\n",
    "# tfidf\n",
    "train_question1_tfidf = pd.read_pickle('train_question1_tfidf.pkl')[:]\n",
    "test_question1_tfidf = pd.read_pickle('test_question1_tfidf.pkl')[:]\n",
    "\n",
    "train_question2_tfidf = pd.read_pickle('train_question2_tfidf.pkl')[:]\n",
    "test_question2_tfidf = pd.read_pickle('test_question2_tfidf.pkl')[:]\n",
    "\n",
    "\n",
    "train_question1_porter_tfidf = pd.read_pickle('train_question1_porter_tfidf.pkl')[:]\n",
    "test_question1_porter_tfidf = pd.read_pickle('test_question1_porter_tfidf.pkl')[:]\n",
    "\n",
    "train_question2_porter_tfidf = pd.read_pickle('train_question2_porter_tfidf.pkl')[:]\n",
    "test_question2_porter_tfidf = pd.read_pickle('test_question2_porter_tfidf.pkl')[:]\n",
    "\n",
    "\n",
    "train_interaction = pd.read_pickle(path+'train_interaction.pkl')[:].reshape(-1,1)\n",
    "test_interaction = pd.read_pickle(path+'test_interaction.pkl')[:].reshape(-1,1)\n",
    "\n",
    "train_porter_interaction = pd.read_pickle(path+'train_porter_interaction.pkl')[:].reshape(-1,1)\n",
    "test_porter_interaction = pd.read_pickle(path+'test_porter_interaction.pkl')[:].reshape(-1,1)\n",
    "\n",
    "\n",
    "train_jaccard = pd.read_pickle('train_jaccard.pkl')[:].reshape(-1,1)\n",
    "test_jaccard = pd.read_pickle('test_jaccard.pkl')[:].reshape(-1,1)\n",
    "\n",
    "train_porter_jaccard = pd.read_pickle('train_porter_jaccard.pkl')[:].reshape(-1,1)\n",
    "test_porter_jaccard = pd.read_pickle('test_porter_jaccard.pkl')[:].reshape(-1,1)\n",
    "\n",
    "train_len = pd.read_pickle(\"train_len.pkl\")\n",
    "test_len = pd.read_pickle(\"test_len.pkl\")\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(np.vstack([train_len,test_len]))\n",
    "train_len = scaler.transform(train_len)\n",
    "test_len =scaler.transform(test_len)\n",
    "\n",
    "\n",
    "X = ssp.hstack([\n",
    "    train_question1_tfidf,\n",
    "    train_question2_tfidf,\n",
    "    train_interaction,\n",
    "    train_porter_interaction,\n",
    "    train_jaccard,\n",
    "    train_porter_jaccard,\n",
    "    train_len,\n",
    "    ]).tocsr()\n",
    "\n",
    "\n",
    "y = train['is_duplicate'].values[:]\n",
    "\n",
    "X_t = ssp.hstack([\n",
    "    test_question1_tfidf,\n",
    "    test_question2_tfidf,\n",
    "    test_interaction,\n",
    "    test_porter_interaction,\n",
    "    test_jaccard,\n",
    "    test_porter_jaccard,\n",
    "    test_len,\n",
    "    ]).tocsr()\n",
    "\n",
    "\n",
    "print (X.shape)\n",
    "print (X_t.shape)\n",
    "\n",
    "skf = KFold(n_splits=5, shuffle=True, random_state=seed).split(X)\n",
    "for ind_tr, ind_te in skf:\n",
    "    X_train = X[ind_tr]\n",
    "    X_test = X[ind_te]\n",
    "\n",
    "    y_train = y[ind_tr]\n",
    "    y_test = y[ind_te]\n",
    "    break\n",
    "\n",
    "dump_svmlight_file(X,y,\"X_tfidf.svm\")\n",
    "del X\n",
    "dump_svmlight_file(X_t,np.zeros(X_t.shape[0]),\"X_t_tfidf.svm\")\n",
    "del X_t\n",
    "\n",
    "def oversample(X_ot,y,p=0.165):\n",
    "    pos_ot = X_ot[y==1]\n",
    "    neg_ot = X_ot[y==0]\n",
    "    #p = 0.165\n",
    "    scale = ((pos_ot.shape[0]*1.0 / (pos_ot.shape[0] + neg_ot.shape[0])) / p) - 1\n",
    "    while scale > 1:\n",
    "        neg_ot = ssp.vstack([neg_ot, neg_ot]).tocsr()\n",
    "        scale -=1\n",
    "    neg_ot = ssp.vstack([neg_ot, neg_ot[:int(scale * neg_ot.shape[0])]]).tocsr()\n",
    "    ot = ssp.vstack([pos_ot, neg_ot]).tocsr()\n",
    "    y=np.zeros(ot.shape[0])\n",
    "    y[:pos_ot.shape[0]]=1.0\n",
    "    print (y.mean())\n",
    "    return ot,y\n",
    "\n",
    "X_train,y_train = oversample(X_train.tocsr(),y_train,p=0.165)\n",
    "X_test,y_test = oversample(X_test.tocsr(),y_test,p=0.165)\n",
    "\n",
    "X_train,y_train = shuffle(X_train,y_train,random_state=seed)\n",
    "\n",
    "dump_svmlight_file(X_train,y_train,\"X_train_tfidf.svm\")\n",
    "dump_svmlight_file(X_test,y_test,\"X_test_tfidf.svm\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from csv import DictReader\n",
    "from math import exp, log, sqrt,pow\n",
    "import itertools\n",
    "import math\n",
    "from random import random,shuffle,uniform,seed\n",
    "import pickle\n",
    "import sys\n",
    "\n",
    "seed(1024)\n",
    "\n",
    "def data_generator(path,no_norm=False,task='c'):\n",
    "    data = open(path,'r')\n",
    "    for row in data:\n",
    "        row = row.strip().split(\" \")\n",
    "        y = float(row[0])\n",
    "        row = row[1:]\n",
    "        x = []\n",
    "        for feature in row:\n",
    "            feature = feature.split(\":\")\n",
    "            idx = int(feature[0])\n",
    "            value = float(feature[1])\n",
    "            x.append([idx,value])\n",
    "\n",
    "        if not no_norm:\n",
    "            r = 0.0\n",
    "            for i in range(len(x)):\n",
    "                r+=x[i][1]*x[i][1]\n",
    "            for i in range(len(x)):\n",
    "                x[i][1] /=r\n",
    "        # if task=='c':\n",
    "        #     if y ==0.0:\n",
    "        #         y = -1.0\n",
    "\n",
    "        yield x,y\n",
    "\n",
    "\n",
    "def dot(u,v):\n",
    "    u_v = 0.\n",
    "    len_u = len(u)\n",
    "    for idx in range(len_u):\n",
    "        uu = u[idx]\n",
    "        vv = v[idx]\n",
    "        u_v+=uu*vv\n",
    "    return u_v\n",
    "\n",
    "def mse_loss_function(y,p):\n",
    "    return (y - p)**2\n",
    "\n",
    "def mae_loss_function(y,p):\n",
    "    y = exp(y)\n",
    "    p = exp(p)\n",
    "    return abs(y - p)\n",
    "\n",
    "def log_loss_function(y,p):\n",
    "    return -(y*log(p)+(1-y)*log(1-p))\n",
    "\n",
    "def exponential_loss_function(y,p):\n",
    "    return log(1+exp(-y*p))\n",
    "\n",
    "def sigmoid(inX):\n",
    "    return 1/(1+exp(-inX))\n",
    "\n",
    "def bounded_sigmoid(inX):\n",
    "    return 1. / (1. + exp(-max(min(inX, 35.), -35.)))\n",
    "\n",
    "\n",
    "class SGD(object):\n",
    "    def __init__(self,lr=0.001,momentum=0.9,nesterov=True,adam=False,l2=0.0,l2_fm=0.0,l2_bias=0.0,ini_stdev= 0.01,dropout=0.5,task='c',n_components=4,nb_epoch=5,interaction=False,no_norm=False):\n",
    "        self.W = []\n",
    "        self.V = []        \n",
    "        self.bias = uniform(-ini_stdev, ini_stdev)\n",
    "        self.n_components=n_components\n",
    "        self.lr = lr\n",
    "        self.l2 = l2\n",
    "        self.l2_fm = l2_fm\n",
    "        self.l2_bias = l2_bias\n",
    "        self.momentum = momentum\n",
    "        self.nesterov = nesterov\n",
    "        self.adam = adam\n",
    "        self.nb_epoch = nb_epoch\n",
    "        self.ini_stdev = ini_stdev\n",
    "        self.task = task\n",
    "        self.interaction = interaction\n",
    "        self.dropout = dropout\n",
    "        self.no_norm = no_norm\n",
    "        if self.task!='c':\n",
    "            # self.loss_function = mse_loss_function\n",
    "            self.loss_function = mae_loss_function\n",
    "        else:\n",
    "            # self.loss_function = exponential_loss_function\n",
    "            self.loss_function = log_loss_function\n",
    "\n",
    "    def preload(self,train,test):\n",
    "        train = data_generator(train,self.no_norm,self.task)\n",
    "        dim = 0\n",
    "        count = 0\n",
    "        for x,y in train:\n",
    "            for i in x:\n",
    "                idx,value = i\n",
    "                if idx >dim:\n",
    "                    dim = idx\n",
    "            count+=1\n",
    "        print('Training samples:',count)\n",
    "        test = data_generator(test,self.no_norm,self.task)\n",
    "        count=0\n",
    "        for x,y in test:\n",
    "            for i in x:\n",
    "                idx,value = i\n",
    "                if idx >dim:\n",
    "                    dim = idx\n",
    "            count+=1\n",
    "        print('Testing samples:',count)\n",
    "        \n",
    "        dim = dim+1\n",
    "        print(\"Number of features:\",dim)\n",
    "        \n",
    "        self.W = [uniform(-self.ini_stdev, self.ini_stdev) for _ in range(dim)]\n",
    "        self.Velocity_W = [0.0 for _ in range(dim)]\n",
    "        \n",
    "        \n",
    "        self.V = [[uniform(-self.ini_stdev, self.ini_stdev) for _ in range(self.n_components)] for _ in range(dim)]\n",
    "        self.Velocity_V = [[0.0 for _ in range(self.n_components)] for _ in range(dim)]\n",
    "        \n",
    "        self.Velocity_bias = 0.0\n",
    "        \n",
    "        self.dim = dim\n",
    "        \n",
    "        \n",
    "    def adam_init(self):\n",
    "        self.iterations = 0\n",
    "        self.beta_1 = 0.9\n",
    "        self.beta_2 = 0.999\n",
    "        self.epsilon=1e-8\n",
    "        self.decay = 0.\n",
    "        self.inital_decay = self.decay \n",
    "\n",
    "        dim =self.dim\n",
    "\n",
    "        self.m_W = [0.0 for _ in range(dim)]\n",
    "        self.v_W = [0.0 for _ in range(dim)]\n",
    "\n",
    "        self.m_V = [[0.0 for _ in range(self.n_components)] for _ in range(dim)]\n",
    "        self.v_V = [[0.0 for _ in range(self.n_components)] for _ in range(dim)]\n",
    "\n",
    "        self.m_bias = 0.0\n",
    "        self.v_bias = 0.0\n",
    "\n",
    "\n",
    "    def adam_update(self,lr,x,residual):\n",
    "\n",
    "        if 0.<self.dropout<1.:\n",
    "            self.droupout_x(x)\n",
    "        \n",
    "        lr = self.lr\n",
    "        if self.inital_decay > 0:\n",
    "            lr *= (1. / (1. + self.decay * self.iterations))\n",
    "\n",
    "        t = self.iterations + 1\n",
    "\n",
    "        lr_t = lr * sqrt(1. - pow(self.beta_2, t)) / (1. - pow(self.beta_1, t))\n",
    "        \n",
    "        for sample in x:\n",
    "            idx,value = sample\n",
    "            g = residual*value\n",
    "\n",
    "            m = self.m_W[idx]\n",
    "            m_t = (self.beta_1 * m) + (1. - self.beta_1) * g\n",
    "\n",
    "            v = self.v_W[idx]\n",
    "            v_t = (self.beta_2 * v) + (1. - self.beta_2) * (g**2)\n",
    "\n",
    "            p = self.W[idx]\n",
    "            p_t = p - lr_t *m_t / (sqrt(v_t) + self.epsilon)\n",
    "\n",
    "            if self.l2>0:\n",
    "                p_t = p_t - lr_t*self.l2*p\n",
    "\n",
    "            self.m_W[idx] = m_t\n",
    "            self.v_W[idx] = v_t\n",
    "            self.W[idx] = p_t\n",
    "\n",
    "        if self.interaction:\n",
    "            self._adam_update_fm(lr_t,x,residual)\n",
    "\n",
    "\n",
    "        m = self.m_bias\n",
    "        m_t = (self.beta_1 * m) + (1. - self.beta_1)*residual\n",
    "\n",
    "        v = self.v_bias\n",
    "        v_t = (self.beta_2 * v) + (1. - self.beta_2)*(residual**2)\n",
    "\n",
    "        p = self.bias\n",
    "        p_t = p - lr_t * m_t / (sqrt(v_t) + self.epsilon)\n",
    "        if self.l2_bias>0:\n",
    "            pt = pt - lr_t * self.l2_bias*p\n",
    "\n",
    "        self.m_bias = m_t\n",
    "        self.v_bias = v_t\n",
    "        self.bias = p_t\n",
    "\n",
    "        self.iterations+=1\n",
    "\n",
    "    def _adam_update_fm(self,lr_t,x,residual):\n",
    "        len_x = len(x)\n",
    "        sum_f_dict = self.sum_f_dict\n",
    "        n_components = self.n_components\n",
    "        for f in range(n_components):\n",
    "            for i in range(len_x):\n",
    "                idx_i,value_i = x[i]\n",
    "                v = self.V[idx_i][f]\n",
    "                sum_f = sum_f_dict[f]\n",
    "                g = (sum_f*value_i - v *value_i*value_i)*residual\n",
    "\n",
    "                m = self.m_V[idx_i][f]\n",
    "                m_t = (self.beta_1 * m) + (1. - self.beta_1) * g\n",
    "\n",
    "                v = self.v_V[idx_i][f]\n",
    "                v_t = (self.beta_2 * v) + (1. - self.beta_2) * (g**2)\n",
    "\n",
    "                p = self.V[idx_i][f]\n",
    "                p_t = p - lr_t * m_t / (sqrt(v_t) + self.epsilon)\n",
    "\n",
    "                if self.l2_fm>0:\n",
    "                    p_t = p_t - lr_t * self.l2_fm*p\n",
    "\n",
    "                self.m_V[idx_i][f] = m_t\n",
    "                self.v_V[idx_i][f] = v_t\n",
    "                self.V[idx_i][f] = p_t\n",
    "\n",
    "    def droupout_x(self,x):\n",
    "        new_x = []\n",
    "        for i, var in enumerate(x):\n",
    "            if random() > self.dropout:\n",
    "                del x[i]\n",
    "\n",
    "    def _predict_fm(self,x):\n",
    "        len_x = len(x)\n",
    "        n_components = self.n_components\n",
    "        pred = 0.0\n",
    "        self.sum_f_dict = {}\n",
    "        for f in range(n_components):\n",
    "            sum_f = 0.0\n",
    "            sum_sqr_f = 0.0\n",
    "            for i in range(len_x):\n",
    "                idx_i,value_i = x[i]\n",
    "                d = self.V[idx_i][f] * value_i\n",
    "                sum_f +=d\n",
    "                sum_sqr_f +=d*d\n",
    "            pred+= 0.5 * (sum_f*sum_f - sum_sqr_f);\n",
    "            self.sum_f_dict[f] = sum_f\n",
    "        return pred\n",
    "\n",
    "    def _predict_one(self,x):\n",
    "        pred = self.bias\n",
    "        # pred = 0.0\n",
    "        for idx,value in x:\n",
    "            pred+=self.W[idx]*value\n",
    "        \n",
    "        if self.interaction:\n",
    "            pred+=self._predict_fm(x)\n",
    "\n",
    "        if self.task=='c':\n",
    "            pred = bounded_sigmoid(pred)\n",
    "        return pred\n",
    "\n",
    "\n",
    "    def _update_fm(self,lr,x,residual):\n",
    "        len_x = len(x)\n",
    "        sum_f_dict = self.sum_f_dict\n",
    "        n_components = self.n_components\n",
    "        for f in range(n_components):\n",
    "            for i in range(len_x):\n",
    "                idx_i,value_i = x[i]\n",
    "                sum_f = sum_f_dict[f]\n",
    "                v = self.V[idx_i][f]\n",
    "                grad = (sum_f*value_i - v *value_i*value_i)*residual\n",
    "                \n",
    "                self.Velocity_V[idx_i][f] = self.momentum * self.Velocity_V[idx_i][f] - lr * grad\n",
    "                if self.nesterov:\n",
    "                    self.Velocity_V[idx_i][f] = self.momentum * self.Velocity_V[idx_i][f] - lr * grad\n",
    "                self.V[idx_i][f] = self.V[idx_i][f] + self.Velocity_V[idx_i][f] - lr*self.l2_fm*self.V[idx_i][f]\n",
    "\n",
    "\n",
    "\n",
    "    def update(self,lr,x,residual):\n",
    "\n",
    "        if 0.<self.dropout<1.:\n",
    "            self.droupout_x(x)\n",
    "\n",
    "        for sample in x:\n",
    "            idx,value = sample\n",
    "            grad = residual*value\n",
    "            self.Velocity_W[idx] =  self.momentum * self.Velocity_W[idx] - lr * grad\n",
    "            if self.nesterov:\n",
    "                 self.Velocity_W[idx] = self.momentum * self.Velocity_W[idx] - lr * grad\n",
    "            self.W[idx] = self.W[idx] + self.Velocity_W[idx] - lr*self.l2*self.W[idx]\n",
    "            \n",
    "        if self.interaction:\n",
    "            self._update_fm(lr,x,residual)\n",
    "\n",
    "        self.Velocity_bias = self.momentum*self.Velocity_bias - lr*residual\n",
    "        if self.nesterov:\n",
    "            self.Velocity_bias = self.momentum*self.Velocity_bias - lr*residual\n",
    "        self.bias = self.bias +self.Velocity_bias - lr*self.l2_bias*self.bias\n",
    "\n",
    "    def predict(self,path,out):\n",
    "\n",
    "        data = data_generator(path,self.no_norm,self.task)\n",
    "        y_preds =[]\n",
    "        with open(out, 'w') as outfile:\n",
    "            ID = 0\n",
    "            outfile.write('%s,%s\\n' % ('test_id', 'is_duplicate'))\n",
    "            for d in data:\n",
    "                x,y = d\n",
    "                p = self._predict_one(x)\n",
    "                outfile.write('%s,%s\\n' % (ID, str(p)))\n",
    "                ID+=1\n",
    "\n",
    "\n",
    "    def validate(self,path):\n",
    "        data = data_generator(path,self.no_norm,self.task)\n",
    "        loss = 0.0\n",
    "        count = 0.0\n",
    "\n",
    "        for d in data:\n",
    "            x,y = d\n",
    "            p = self._predict_one(x)\n",
    "            loss+=self.loss_function(y,p)\n",
    "            count+=1\n",
    "        return loss/count\n",
    "\n",
    "    def save_weights(self):\n",
    "        weights = []\n",
    "        weights.append(self.W)\n",
    "        weights.append(self.V)\n",
    "        weights.append(self.bias)\n",
    "        # weights.append(self.Velocity_W)\n",
    "        # weights.append(self.Velocity_V)\n",
    "        weights.append(self.dim)\n",
    "        pickle.dump(weights,open('sgd_fm.pkl','wb'))\n",
    "\n",
    "    def load_weights(self):\n",
    "        weights = pickle.load(open('sgd_fm.pkl','rb'))\n",
    "        self.W = weights[0]\n",
    "        self.V = weights[1]\n",
    "        self.bias = weights[2]\n",
    "        # self.Velocity_W = weights[3]\n",
    "        # self.Velocity_V = weights[4]\n",
    "        self.dim = weights[3]\n",
    "        \n",
    "\n",
    "    def train(self,path,valid_path = None,in_memory=False):\n",
    "\n",
    "        start = datetime.now()\n",
    "        lr = self.lr\n",
    "        if self.adam:\n",
    "            self.adam_init()\n",
    "            self.update = self.adam_update\n",
    "\n",
    "        if in_memory:\n",
    "            data = data_generator(path,self.no_norm,self.task)\n",
    "            data = [d for d in data]\n",
    "        best_loss = 999999\n",
    "        best_epoch = 0\n",
    "        for epoch in range(1,self.nb_epoch+1):\n",
    "            if not in_memory:\n",
    "                data = data_generator(path,self.no_norm,self.task)\n",
    "            train_loss = 0.0\n",
    "            train_count = 0\n",
    "            for x,y in data:\n",
    "                p = self._predict_one(x)\n",
    "                if self.task!='c':                    \n",
    "                    residual = -(y-p)\n",
    "                else:\n",
    "                    # residual = -y*(1.0-1.0/(1.0+exp(-y*p)));\n",
    "                    residual = -(y-p)\n",
    "\n",
    "                self.update(lr,x,residual)\n",
    "                #if train_count%50000==0:\n",
    "                    #if train_count ==0:\n",
    "                        #print \"\\ttrain_count: %s, current loss: %.6f \" %(train_count,0.0)\n",
    "                    #else:\n",
    "                        #print \"\\ttrain_count: %s, current loss: %.6f \" %(train_count,train_loss/train_count)\n",
    "\n",
    "                train_loss += self.loss_function(y,p)\n",
    "                train_count += 1\n",
    "\n",
    "            epoch_end = datetime.now()\n",
    "            duration = epoch_end-start\n",
    "            \n",
    "            if valid_path:\n",
    "                valid_loss = self.validate(valid_path)\n",
    "                print('Epoch: %s, train loss: %.6f, valid loss: %.6f, time: %s'%(epoch,train_loss/train_count,valid_loss,duration))\n",
    "                if valid_loss<best_loss:\n",
    "                    best_loss = valid_loss\n",
    "                    self.save_weights()\n",
    "                    print ('save_weights')\n",
    "            else:\n",
    "                print('Epoch: %s, train loss: %.6f, time: %s'%(epoch,train_loss/train_count,duration))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.26653782827639033\n"
     ]
    }
   ],
   "source": [
    "#path = \"/input/\"\n",
    "\n",
    "\n",
    "#sgd = SGD(lr=0.001,adam=True,dropout=0.9,l2=0.00,l2_fm=0.00,task='c',n_components=1,nb_epoch=7,interaction=True,no_norm=False)\n",
    "#sgd.preload('X_tfidf.svm','X_t_tfidf.svm')\n",
    "#sgd.load_weights()\n",
    "#sgd.train('X_train_tfidf.svm','X_test_tfidf.svm',in_memory=False)\n",
    "#sgd.load_weights()\n",
    "sgd.predict('X_test_tfidf.svm',out='valid.csv')\n",
    "print (sgd.validate('X_test_tfidf.svm'))\n",
    "sgd.predict('X_t_tfidf.svm',out='out.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 404290\n",
      "Testing samples: 2345796\n",
      "Number of features: 3073539\n",
      "Epoch: 1, train loss: 0.178870, valid loss: 0.265899, time: 0:03:38.888776\n",
      "save_weights\n",
      "Epoch: 2, train loss: 0.176230, valid loss: 0.265764, time: 0:07:36.201836\n",
      "save_weights\n",
      "Epoch: 3, train loss: 0.174003, valid loss: 0.265822, time: 0:11:32.787598\n",
      "Epoch: 4, train loss: 0.171890, valid loss: 0.265844, time: 0:15:23.711215\n",
      "Epoch: 5, train loss: 0.169880, valid loss: 0.265811, time: 0:19:14.039829\n",
      "Epoch: 6, train loss: 0.167945, valid loss: 0.265925, time: 0:23:03.875314\n",
      "Epoch: 7, train loss: 0.166068, valid loss: 0.266148, time: 0:26:54.242594\n"
     ]
    }
   ],
   "source": [
    "sgd = SGD(lr=0.0001,adam=True,dropout=0.9,l2=0.00,l2_fm=0.00,task='c',n_components=1,nb_epoch=7,interaction=True,no_norm=False)\n",
    "sgd.preload('X_tfidf.svm','X_t_tfidf.svm')\n",
    "sgd.load_weights()\n",
    "sgd.train('X_train_tfidf.svm','X_test_tfidf.svm',in_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 404290\n",
      "Testing samples: 2345796\n",
      "Number of features: 3073539\n",
      "Epoch: 1, train loss: 0.174711, valid loss: 0.266495, time: 0:03:36.902795\n",
      "save_weights\n",
      "Epoch: 2, train loss: 0.174439, valid loss: 0.266531, time: 0:07:35.357475\n",
      "Epoch: 3, train loss: 0.174205, valid loss: 0.266523, time: 0:11:29.261616\n",
      "Epoch: 4, train loss: 0.173984, valid loss: 0.266506, time: 0:15:23.453916\n",
      "Epoch: 5, train loss: 0.173771, valid loss: 0.266516, time: 0:19:17.503942\n",
      "Epoch: 6, train loss: 0.173565, valid loss: 0.266513, time: 0:23:11.625396\n",
      "Epoch: 7, train loss: 0.173366, valid loss: 0.266519, time: 0:27:05.481680\n"
     ]
    }
   ],
   "source": [
    "sgd = SGD(lr=0.00001,adam=True,dropout=0.7,l2=0.00,l2_fm=0.00,task='c',n_components=1,nb_epoch=7,interaction=True,no_norm=False)\n",
    "sgd.preload('X_tfidf.svm','X_t_tfidf.svm')\n",
    "sgd.load_weights()\n",
    "sgd.train('X_train_tfidf.svm','X_test_tfidf.svm',in_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 404290\n",
      "Testing samples: 2345796\n",
      "Number of features: 3073539\n",
      "Epoch: 1, train loss: 0.174483, valid loss: 0.266570, time: 0:03:20.744350\n",
      "save_weights\n",
      "Epoch: 2, train loss: 0.174263, valid loss: 0.266513, time: 0:07:01.424527\n",
      "save_weights\n"
     ]
    }
   ],
   "source": [
    "sgd = SGD(lr=0.00001,adam=True,dropout=0.7,l2=0.00,l2_fm=0.00,task='c',n_components=1,nb_epoch=2,interaction=True,no_norm=False)\n",
    "sgd.preload('X_tfidf.svm','X_t_tfidf.svm')\n",
    "sgd.load_weights()\n",
    "sgd.train('X_train_tfidf.svm','X_test_tfidf.svm',in_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 404290\n",
      "Testing samples: 2345796\n",
      "Number of features: 3073539\n",
      "Epoch: 1, train loss: 0.173203, valid loss: 0.266596, time: 0:03:12.391957\n",
      "save_weights\n",
      "Epoch: 2, train loss: 0.172988, valid loss: 0.266499, time: 0:06:43.702866\n",
      "save_weights\n",
      "Epoch: 3, train loss: 0.172798, valid loss: 0.266525, time: 0:10:14.751867\n",
      "Epoch: 4, train loss: 0.172615, valid loss: 0.266511, time: 0:13:43.394286\n",
      "Epoch: 5, train loss: 0.172438, valid loss: 0.266534, time: 0:17:11.608997\n",
      "Epoch: 6, train loss: 0.172268, valid loss: 0.266538, time: 0:20:40.233964\n"
     ]
    }
   ],
   "source": [
    "sgd = SGD(lr=0.00001,adam=True,momentum=0.5,dropout=0.7,l2=0.00,l2_fm=0.00,task='c',n_components=1,nb_epoch=6,interaction=True,no_norm=False)\n",
    "sgd.preload('X_tfidf.svm','X_t_tfidf.svm')\n",
    "sgd.load_weights()\n",
    "sgd.train('X_train_tfidf.svm','X_test_tfidf.svm',in_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting requests\n",
      "  Downloading requests-2.13.0-py2.py3-none-any.whl (584kB)\n",
      "\u001b[K    100% |████████████████████████████████| 593kB 1.9MB/s ta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: requests\n",
      "Successfully installed requests-2.13.0\n"
     ]
    }
   ],
   "source": [
    "!pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def download_file_from_google_drive(id, destination):\n",
    "    URL = \"https://drive.google.com/uc?id=0B16OiWl7I-fBZmRlZnAtZmt5OVk\"\n",
    "\n",
    "    session = requests.Session()\n",
    "\n",
    "    response = session.get(URL, params = { 'id' : id }, stream = True)\n",
    "    token = get_confirm_token(response)\n",
    "\n",
    "    if token:\n",
    "        params = { 'id' : id, 'confirm' : token }\n",
    "        response = session.get(URL, params = params, stream = True)\n",
    "\n",
    "    save_response_content(response, destination)    \n",
    "\n",
    "def get_confirm_token(response):\n",
    "    for key, value in response.cookies.items():\n",
    "        if key.startswith('download_warning'):\n",
    "            return value\n",
    "\n",
    "    return None\n",
    "\n",
    "def save_response_content(response, destination):\n",
    "    CHUNK_SIZE = 32768\n",
    "\n",
    "    with open(destination, \"wb\") as f:\n",
    "        for chunk in response.iter_content(CHUNK_SIZE):\n",
    "            if chunk: # filter out keep-alive new chunks\n",
    "                f.write(chunk)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    file_id = '0B16OiWl7I-fBZmRlZnAtZmt5OVk'\n",
    "    destination = 'svm.zip'\n",
    "    download_file_from_google_drive(file_id, destination)\n",
    "    #https://drive.google.com/uc?id=0B16OiWl7I-fBZmRlZnAtZmt5OVk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def download_file_from_google_drive(id, destination):\n",
    "    URL = \"https://drive.google.com/uc?id=0B16OiWl7I-fBZ21iS2Z1SFY4Rmc\"\n",
    "\n",
    "    session = requests.Session()\n",
    "\n",
    "    response = session.get(URL, params = { 'id' : id }, stream = True)\n",
    "    token = get_confirm_token(response)\n",
    "\n",
    "    if token:\n",
    "        params = { 'id' : id, 'confirm' : token }\n",
    "        response = session.get(URL, params = params, stream = True)\n",
    "\n",
    "    save_response_content(response, destination)    \n",
    "\n",
    "def get_confirm_token(response):\n",
    "    for key, value in response.cookies.items():\n",
    "        if key.startswith('download_warning'):\n",
    "            return value\n",
    "\n",
    "    return None\n",
    "\n",
    "def save_response_content(response, destination):\n",
    "    CHUNK_SIZE = 32768\n",
    "\n",
    "    with open(destination, \"wb\") as f:\n",
    "        for chunk in response.iter_content(CHUNK_SIZE):\n",
    "            if chunk: # filter out keep-alive new chunks\n",
    "                f.write(chunk)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    file_id = '0B16OiWl7I-fBZ21iS2Z1SFY4Rmc'\n",
    "    destination = 'pkl.zip'\n",
    "    download_file_from_google_drive(file_id, destination)\n",
    "    #https://drive.google.com/open?id=0B16OiWl7I-fBZ21iS2Z1SFY4Rmc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  svm.zip\n",
      "  inflating: X_test_tfidf.svm        \n",
      "  inflating: X_tfidf.svm             \n",
      "  inflating: X_train_tfidf.svm       \n",
      "  inflating: X_t_tfidf.svm           \n",
      "Archive:  pkl.zip\n",
      "  inflating: train_question1_porter_tfidf.pkl  \n",
      "  inflating: train_question1_tfidf.pkl  \n",
      "  inflating: train_question2_porter_tfidf.pkl  \n",
      "  inflating: train_question2_tfidf.pkl  \n",
      "  inflating: sgd_fm.pkl              \n",
      "  inflating: test_interaction.pkl    \n",
      "  inflating: test_jaccard.pkl        \n",
      "  inflating: test_len.pkl            \n",
      "  inflating: test_porter_interaction.pkl  \n",
      "  inflating: test_porter_jaccard.pkl  \n",
      "  inflating: test_question1_porter_tfidf.pkl  \n",
      "  inflating: test_question1_tfidf.pkl  \n",
      "  inflating: test_question2_porter_tfidf.pkl  \n",
      "  inflating: test_question2_tfidf.pkl  \n",
      "  inflating: train_interaction.pkl   \n",
      "  inflating: train_jaccard.pkl       \n",
      "  inflating: train_len.pkl           \n",
      "  inflating: train_porter_interaction.pkl  \n",
      "  inflating: train_porter_jaccard.pkl  \n"
     ]
    }
   ],
   "source": [
    "!unzip svm.zip\n",
    "!unzip pkl.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
